{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import json \n",
    "import re\n",
    "import sys \n",
    "import datetime \n",
    "import requests \n",
    "import os \n",
    "import time \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get url for each apartment\n",
    "def get_apt_url(main_web):\n",
    "    \"\"\"\n",
    "    main_web_url: main web url for apartments.com, i.e., 'https://www.apartments.com/boston-ma/'\n",
    "    return: url for each apartment \n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "    \n",
    "    # find total pages \n",
    "    response = requests.get(main_web, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    soup.prettify()\n",
    "    soup = soup.find('div', class_='placardContainer')\n",
    "    soup = soup.ul.find('nav', class_='paging')\n",
    "    \n",
    "    pages = []\n",
    "    for page in soup.find_all('a'):\n",
    "        pages.append(page.get('data-page'))\n",
    "        pages = list(map(int, pages))\n",
    "    print('{} relevant numbers found, max page number is {}'.format(len(pages), max(pages)))\n",
    "    \n",
    "    # total pages found \n",
    "    last_page = max(pages)\n",
    "    \n",
    "    # add suffix to get whole url for all pages \n",
    "    urls = []\n",
    "    for i in range(1, last_page+1):\n",
    "        urls.append(''.join([main_web, str(i), '/']))\n",
    "\n",
    "    print('Example url looks like {}'.format(urls[0]))\n",
    "    \n",
    "    # test request return status code \n",
    "    for url in urls:\n",
    "        print('For url {}, status code is {}'\\\n",
    "              .format(url, requests.get(url, headers=headers).status_code))\n",
    "        \n",
    "    # get url for each apartment in each page \n",
    "    apt_urls = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup=BeautifulSoup(response.content, 'lxml')\n",
    "        soup.prettify()\n",
    "        soup = soup.find('div', class_='placardContainer')\n",
    "        \n",
    "        for item in soup.find_all('article', class_='placard'):\n",
    "            # add random gap between each request \n",
    "            time.sleep(np.random.uniform(low=5, high=20))\n",
    "            \n",
    "            if item.find('a', class_='placardTitle js-placardTitle ') is None:\n",
    "                continue \n",
    "            else:\n",
    "                apt_urls.append(item.find('a', class_='placardTitle js-placardTitle ').get('href'))\n",
    "                print('Parsing in progress: {}'.format(len(apt_urls)))\n",
    "        print('{} apt urls got!'.format(len(apt_urls)))\n",
    "\n",
    "    return apt_urls \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apt_urls = get_apt_url('https://www.apartments.com/boston-ma/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_url = pd.DataFrame({'apt_url':apt_urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "def build_dataset(apt_urls):\n",
    "    \"\"\"\n",
    "    apt_url: url of each apartment/house/townhouse/condo \n",
    "    return: data frame populated with entity information \n",
    "    \"\"\"\n",
    "    # feature names \n",
    "    cols = [\n",
    "        'name',\n",
    "        'address',\n",
    "        'bd',\n",
    "        'ba',\n",
    "        'sqft',\n",
    "        'rent',\n",
    "        'desc',\n",
    "        'pet',\n",
    "        'parking',\n",
    "        'pubSchool',\n",
    "        'privSchool',\n",
    "        'walk',\n",
    "        'transit',\n",
    "        'ptype',\n",
    "        'numPOI',\n",
    "        'nearCollege',\n",
    "        'distNearCollege',\n",
    "        'numCollege',\n",
    "        'distNearSubway',\n",
    "        'numSubway',\n",
    "        'distNearRail',\n",
    "        'numRail',\n",
    "        'distNearShop',\n",
    "        'numShop',\n",
    "        'distNearPark',\n",
    "        'numPark',\n",
    "        'distAirport'\n",
    "    ]\n",
    "    \n",
    "    # create empty dataframe \n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    # used for requests.get()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
    "\n",
    "    # extract information from each apt/house \n",
    "    i = 1 # counter \n",
    "    for apt_url in apt_urls:        \n",
    "        # read web content\n",
    "        #url = 'https://www.apartments.com/10-shepard-st-boston-ma/xe7n4sm/'\n",
    "        response = requests.get(apt_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        soup.prettify()\n",
    "        \n",
    "        print('processing: {}'.format(apt_url))\n",
    "\n",
    "        # get name \n",
    "        name = soup.find('h1', class_='propertyName').getText().strip()\n",
    "\n",
    "        # get address \n",
    "        address = \"\"\n",
    "        add = soup.find('div', class_='propertyAddress').h2\n",
    "\n",
    "        for item in add.find_all('span'):\n",
    "            address += item.getText() + ','   \n",
    "\n",
    "        # bed, bath, rent, sqft \n",
    "        bd = []\n",
    "        ba = []\n",
    "        sqft = []\n",
    "        rent = []\n",
    "\n",
    "        for item in soup.find_all('tr', class_='rentalGridRow'):\n",
    "            if item is not None:\n",
    "                bd_soup = item.find('td', class_='beds')\n",
    "                bd.append(bd_soup.find('span', class_='longText').getText().strip())\n",
    "\n",
    "                ba_soup = item.find('td', class_='baths')\n",
    "                ba.append(ba_soup.find('span', class_='longText').getText().strip())\n",
    "\n",
    "                sqft.append(item.find('td', class_='sqft').getText().strip())\n",
    "\n",
    "                rent.append(item.find('td', class_='rent').getText().strip())\n",
    "            else:\n",
    "                bd = 'missing'\n",
    "                ba = 'missing'\n",
    "                sqft = 'missing'\n",
    "                rent = 'missing' \n",
    "\n",
    "        # description \n",
    "        desc_soup = soup.find('section', class_='descriptionSection js-viewAnalyticsSection')\n",
    "\n",
    "        if desc_soup is None:\n",
    "            desc = 'missing'\n",
    "        else:\n",
    "            desc = desc_soup.p.getText()\n",
    "\n",
    "        # pet \n",
    "        if soup.find('div', class_='petPolicyDetails') is not None:\n",
    "            pet = soup.find('div', class_='petPolicyDetails')\n",
    "            pet = pet.find('span').getText().strip()\n",
    "        else:\n",
    "            pet =  'missing' \n",
    "\n",
    "        # parking \n",
    "        if soup.find('div', class_='parkingDetails') is not None:\n",
    "            parking = soup.find_all('div', 'parkingDetails')\n",
    "            parking = parking[0].find('div', class_='parkingTypeFeeContainer')\n",
    "            parking = parking.h4.getText().strip()\n",
    "        else:\n",
    "            parking = 'missing'\n",
    "\n",
    "        # school section \n",
    "        pubSchool = 0\n",
    "        privSchool = 0\n",
    "\n",
    "        if soup.find('div', class_='schoolsPublicContainer') is not None:\n",
    "            pub = soup.find('div', class_='schoolsPublicContainer')\n",
    "            for item in pub.find_all('div', class_='cell-sm-6 cell-xs-12 paddingReset'):\n",
    "                pubSchool += 1\n",
    "        else:\n",
    "            pubSchool = 'missing'\n",
    "\n",
    "        if soup.find('div', class_='schoolsPrivateContainer') is not None:\n",
    "            priv = soup.find('div', class_='schoolsPrivateContainer')\n",
    "            for item in pub.find_all('div', class_='cell-sm-6 cell-xs-12 paddingReset'):\n",
    "                privSchool += 1\n",
    "        else:\n",
    "            privSchool = 'missing'\n",
    "\n",
    "        # walk score  \n",
    "        if soup.find('div', class_='ratingCol walkScore') is not None:\n",
    "            walk = soup.find('div', class_='ratingCol walkScore')['data-score']\n",
    "        else:\n",
    "            walk = 'missing'\n",
    "\n",
    "        # transit score \n",
    "        if soup.find('div', class_='ratingCol transitScore') is not None:\n",
    "            transit = soup.find('div', class_='ratingCol transitScore')['data-score']\n",
    "        else:\n",
    "            transit = 'missing'\n",
    "\n",
    "        # get property type \n",
    "        if soup.find('div', class_='crumbs') is not None:\n",
    "            ame_soup = soup.find('div', class_='crumbs')\n",
    "            prop_type = ame_soup.find_all('span', 'crumb')\n",
    "            ptype = prop_type[0].a['data-type']\n",
    "        else:\n",
    "            ptype = 'missing'\n",
    "\n",
    "        # point of interst \n",
    "        numPOI = 0\n",
    "        if soup.find('section', class_='pointsOfInterestSection') is not None:\n",
    "            poi = soup.find('section', class_='pointsOfInterestSection')\n",
    "            poi = poi.find_all('div', class_='transportationDetail ')\n",
    "            for item in poi:\n",
    "                numPOI += 1\n",
    "        else:\n",
    "            numPOI = 'missing'\n",
    "            \n",
    "        # nearby college \n",
    "        if soup.find('span', class_='poiSchoolIcon') is None:\n",
    "            nearCollege = 'missing'\n",
    "            distNearCollege = 'missing'\n",
    "            numCollege = 'missing'\n",
    "        else:\n",
    "            son = soup.find('span', class_='poiSchoolIcon')\n",
    "            par = son.find_previous('div', class_='transportationDetail')\n",
    "            \n",
    "            allCollege = par.find_all('div', class_='transportationName')\n",
    "            nearCollege = allCollege[0].a.getText().strip()\n",
    "            distNearCollege = allCollege[0].find_previous('td').find_next_siblings()[1].getText().strip()\n",
    "            \n",
    "        # nearby subway \n",
    "        if soup.find('span', class_='poiTransitIcon') is None:\n",
    "            distNearSubway = 'missing'\n",
    "            numSubway = 'missing'\n",
    "        else:\n",
    "            allTransit = soup.find_all('span', class_='poiTransitIcon')\n",
    "            \n",
    "            \n",
    "\n",
    "        # populate a dataframe \n",
    "        df_new = pd.DataFrame(data={'name':name,\n",
    "                      'address':address,\n",
    "                      'bd':bd,\n",
    "                      'ba':ba,\n",
    "                      'sqft':sqft,\n",
    "                      'rent':rent,\n",
    "                      'desc':desc,\n",
    "                      'pet':pet,\n",
    "                      'parking':parking,\n",
    "                      'pubSchool':pubSchool,\n",
    "                      'privSchool':privSchool,\n",
    "                      'walk':walk,\n",
    "                      'transit':transit,\n",
    "                      'ptype':ptype,\n",
    "                      'numPOI':numPOI})\n",
    "        \n",
    "        \n",
    "        # drop duplicated rows for apartments \n",
    "        df_new.drop_duplicates(inplace=True)\n",
    "        \n",
    "        # drop rows with same bd/ba/sqft/address, but different rent\n",
    "        df_new.drop_duplicates(inplace=True, subset=['bd','ba','sqft'])\n",
    "        \n",
    "        print('#{}, \\n new df: {} \\n total df: {}'.format(i, df_new.shape, df.shape))\n",
    "        i += 1 # counter \n",
    "          \n",
    "        df = df.append(df_new, ignore_index=True)\n",
    "        \n",
    "        # gap between each parsing \n",
    "        gap = np.random.uniform(low=5, high=20)\n",
    "        print('Wait for {} second'.format(gap))\n",
    "        time.sleep(gap)\n",
    "             \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'nearCollege',\n",
    "        'distNearCollege',\n",
    "        'numCollege',\n",
    "        'distNearSubway',\n",
    "        'numSubway',\n",
    "        'distNearRail',\n",
    "        'numRail',\n",
    "        'distNearShop',\n",
    "        'numShop',\n",
    "        'distNearPark',\n",
    "        'numPark',\n",
    "        'distAirport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"\"\"Lantera is a word meaning \"beacon\", and it's the heart and soul of the new 15-acre Boston Landing neighborhood. Apartments feature floor-to-ceiling windows, premium finishes with stainless steel appliances, and technology touches throughout. The heart of Boston is less than five miles away and is easily accessible via the Boston Landing commuter rail stop - just a three-minute walk away. Lantera is surrounded by eclectic restaurants, curated retail shops and world-class athletic facilities.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
